{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-ray-train)=\n",
    "\n",
    "# Ray Train\n",
    "\n",
    "Ray Train uses Ray's Actor and Task to support the machine learning and deep learning training processes, and implements the horizontal expansion of single-machine tasks. In short, in Ray Train , each Actor has an independent copy of the machine learning model and can complete the training task independently. Using the horizontal expansion capability of Actors, Ray Train enables training tasks to be expanded on Ray clusters.\n",
    "\n",
    "Ray Train encapsulates common machine learning libraries such as PyTorch, PyTorch Lightning, HuggingFace Transformers, XGBoost, LightGBM, and provides interfaces to users. Users do not need to write Actor code, and only need to make few modifications to the original single-machine machine learning workflow to quickly switch to cluster mode. Taking PyTorch as an example, this section describes how to achieve horizontal expansion of training tasks based on data parallelism. For details on the principle of data parallelism, see {numref}`sec-data-parallel`.\n",
    "\n",
    "## Key steps\n",
    "\n",
    "To modify a PyTorch stand-alone training code to run on Ray Train, the following changes need to be made:\n",
    "\n",
    "* Define `train_loop`, which is a single-node training function, including loading data and updating parameters.\n",
    "\n",
    "* Define [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html), which defines how to scale this training job horizontally, including how many computing nodes are needed, whether to use GPU, etc.\n",
    "\n",
    "* Define `Trainer`, glue `train_loop` and `ScalingConfig` together, and then execute the `Trainer.fit()` method for training.\n",
    "\n",
    "{numref}`fig-ray-train-key-parts` shows the key parts of adapting Ray Train.\n",
    "\n",
    "```{figure} ../img/ch-ray-ml/ray-train-key-parts.svg\n",
    "---\n",
    "width: 500px\n",
    "name: fig-ray-train-key-parts\n",
    "---\n",
    "Ray Train Key Parts\n",
    "```\n",
    "\n",
    "The codes mainly include:\n",
    "\n",
    "```python\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "def train_loop():\n",
    "    ...\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=..., use_gpu=...)\n",
    "trainer = TorchTrainer(train_loop_per_worker=train_loop, scaling_config=scaling_config)\n",
    "result = trainer.fit()\n",
    "```\n",
    "\n",
    "## Example: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a complete training example. This example uses the ResNet model provided by PyTorch {cite}`he2016DeepResidualLearning`. Readers can set `ScalingConfig` based on the number of GPUs in their environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import ray\n",
    "import ray.train.torch\n",
    "from ray.train import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model, optimizer, criterion, train_loader):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # No need to manually send images and labels to a specific GPU\n",
    "        # `prepare_data_loader` helps with this process\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_func(model, data_loader):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            # data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"../data\")\n",
    "\n",
    "def train_loop():\n",
    "    # Load data and perform data augmentation\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(), \n",
    "         torchvision.transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "\n",
    "    # 1. Distribute data to multiple computing nodes\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    test_loader = ray.train.torch.prepare_data_loader(test_loader)\n",
    "    \n",
    "    # The original resnet is designed for 3-channel images\n",
    "    # FashionMNIST is 1 channel, modify the first layer of resnet to adapt to this input\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    \n",
    "    # 2. Distribute the model to multiple computing nodes and GPUs\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train for 10 epochs\n",
    "    for epoch in range(10):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_func(model, optimizer, criterion, train_loader)\n",
    "        acc = test_func(model, test_loader)\n",
    "        \n",
    "        # 3. Monitor training metrics and save checkpoints\n",
    "        metrics = {\"acc\": acc, \"epoch\": epoch}\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics,\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-10 09:41:32</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:33.99        </td></tr>\n",
       "<tr><td>Memory:      </td><td>31.5/90.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/64 CPUs, 4.0/4 GPUs (0.0/1.0 accelerator_type:TITAN)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   acc</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_3d3d1_00000</td><td>TERMINATED</td><td>10.0.0.3:49324</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         80.9687</td><td style=\"text-align: right;\">0.8976</td><td style=\"text-align: right;\">      9</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(RayTrainWorker pid=49400)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49399) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49400) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49401) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49402) world_rank=3, local_rank=3, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m [rank2]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=49400)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=49402)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=49402)\u001b[0m [rank3]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8604, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8808, 'epoch': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000002)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8852, 'epoch': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000003)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8964, 'epoch': 3}\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8972, 'epoch': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000004)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000005)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8968, 'epoch': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000006)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8948, 'epoch': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000007)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.894, 'epoch': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000008)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.894, 'epoch': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000009)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8976, 'epoch': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 09:41:32,109\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-04-10 09:41:32,112\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name' in 0.0057s.\n",
      "2024-04-10 09:41:32,120\tINFO tune.py:1048 -- Total run time: 94.05 seconds (93.99 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# 4. Configure `ScalingConfig`, Ray Train will expand the training task to the cluster according to this configuration\n",
    "scaling_config = ray.train.ScalingConfig(num_workers=4, use_gpu=True)\n",
    "\n",
    "# 5. Start parallel training using TorchTrainer\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=os.path.join(data_dir, \"torch_ckpt\"),\n",
    "        name=\"exp_fashionmnist_resnet18\",\n",
    "    )\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference from native PyTorch\n",
    "\n",
    "### Difference from stand-alone program\n",
    "\n",
    "Ray Train helps users distribute models and data to multiple computing nodes. Users need to set: `model = ray.train.torch.prepare_model(model)` and `train_loader = ray.train.torch.prepare_data_loader(train_loader)`. After setting, Ray Train does not need to explicitly call `model.to(\"cuda\")`, nor does it need `images, labels = images.to(\"cuda\"), labels.to(\"cuda\")` and other codes to copy model data to GPU.\n",
    "\n",
    "### Difference from `DistributedDataParallel`\n",
    "\n",
    "PyTorch's `DistributedDataParallel` can also implement data parallelism. Ray Train hides the complex details in `DistributedDataParallel`, and only requires users to make slight changes from the stand-alone code. The distributed environment (World) and process (Rank) of `torch.distributed` are not needed. For concepts such as World and Rank, please refer to {numref}`sec-mpi-hello-world`.\n",
    "\n",
    "## Data reading\n",
    "\n",
    "If the stand-alone version of data reading is based on PyTorch's `DataLoader`, you can use [`ray.train.torch.prepare_data_loader()`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html) to adapt the original PyTorch `DataLoader`. You can also use the data preprocessing method provided by Ray Data for data preprocessing.\n",
    "\n",
    "## `ScalingConfig`\n",
    "\n",
    "The `num_workers` parameter in `ScalingConfig(num_workers=..., use_gpu=...)` is used to control the parallelism of the task, and the `use_gpu` parameter is used to control whether GPU resources are used. `num_workers` can be understood as the number of Ray Actors started, each of which performs training tasks independently. If `use_gpu=True`, by default, each Actor will be assigned 1 GPU, and accordingly, the environment variable `CUDA_VISIBLE_DEVICES` for each Actor is also 1. To enable each Actor to access multiple GPUs, you can set the `resources_per_worker` parameter: `resources_per_worker={\"GPU\": n}`.\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "In distributed training, each Worker runs independently, but in most cases, you only need to monitor the first process with a process number (Rank) of 0. `ray.train.report(metrics=...)` collects metrics for Rank=0 by default.\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "The Checkpoint process is as follows:\n",
    "\n",
    "1. Checkpoint will be written to a local directory first. You can directly use the model saving interface provided by PyTorch, PyTorch Lightning or TensorFlow. For example, in the example above:\n",
    "\n",
    "```\n",
    "with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "    torch.save(\n",
    "        model.module.state_dict(),\n",
    "        os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "    )\n",
    "```\n",
    "\n",
    "1. When `ray.train.report(metrics=..., checkpoint=...)` is called, the newly saved local checkpoint is uploaded to a persistent file system (e.g., S3 or HDFS), which is accessible to all compute nodes. The local checkpoint is just a cache. After the checkpoint is uploaded to the persistent file system, the local checkpoint will be deleted. The persistent file system directory is configured on `TorchTrainer`:\n",
    "\n",
    "```{code-block} python\n",
    ":name: ray-train-persistent-storage\n",
    ":emphasize-lines: 5\n",
    "\n",
    "TorchTrainer(\n",
    "    train_loop,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=...,\n",
    "        name=\"experiment_name\",\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "When using data parallel training, each rank has a copy of the model weights, which are saved locally and are the same as the checkpoints on the persistent file system. When using other parallel strategies such as pipeline parallel training ({numref}`sec-pipeline-parallel`), each rank saves part of the model locally, and each rank saves its own part of the model weights. When generating checkpoint files, some file prefixes and suffixes should be added to distinguish them.\n",
    "\n",
    "```{code-block} python\n",
    ":name: ray-train-distributed-checkpoint\n",
    ":emphasize-lines: 2,5\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "    rank = train.get_context().get_world_rank()\n",
    "    torch.save(\n",
    "        ...,\n",
    "        os.path.join(temp_checkpoint_dir, f\"model-rank={rank}.pt\"),\n",
    "    )\n",
    "    train.report(\n",
    "        metrics, \n",
    "        checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
