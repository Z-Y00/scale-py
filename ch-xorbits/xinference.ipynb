{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-xinference)=\n",
    "# Xinference\n",
    "\n",
    "Xorbits Inference (Xinference) is an inference platform for large models, supporting large language models, vector models, and text-to-image models. It is based on the distributed computation provided by [Xoscar](https://github.com/xorbitsai/xoscar), allowing models to be deployed on a cluster. The platform offers an OpenAI-like interface, enabling users to deploy and call open-source large models. Xinference integrates the API for external services, inference engine, and hardware, eliminating the need to write code to manage model inference services like Ray Serve.\n",
    "\n",
    "## Inference Engine\n",
    "\n",
    "Xinference can adapt to different inference engines, including Hugging Face Transformers, [vLLM](https://github.com/vllm-project/vllm), [llama.cpp](https://github.com/ggerganov/llama.cpp), etc. Therefore, you need to install the corresponding inference engine during installation, such as `pip install \"xinference[transformers]\"`. Transformers is entirely based on PyTorch, offering the fastest and most comprehensive model compatibility, but with poorer performance; other inference engines, such as vLLM and llama.cpp, focus on performance optimization but do not cover as many models as Transformers.\n",
    "\n",
    "## Cluster\n",
    "\n",
    "Before using, you need to start a Xinference cluster, which can be either single-machine multi-GPU or multi-machine multi-GPU. On a single machine, you can start it from the command line like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "xinference-local --host 0.0.0.0 --port 9997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster setup is similar to Xorbits Data. First, start a Supervisor, then start the Worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Start the Supervisor\n",
    "xinference-supervisor -H <supervisor_ip>\n",
    "\n",
    "# Start the Worker\n",
    "xinference-worker -e \"http://<supervisor_ip>:9997\" -H <worker_ip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, you can access the Xinference service at http://<supervisor_ip>:9997.\n",
    "\n",
    "## Using Models\n",
    "\n",
    "Xinference provides full lifecycle management for models, including starting, running, and shutting down models. Once the Xinference service is started, users can start and use models. Xinference supports various open-source models, allowing users to select and start models through a web interface. Xinference will automatically download and initialize the required models in the backend. Each model comes with a web-based conversation interface and provides an OpenAI API-compatible interface.\n",
    "\n",
    "Next, we will demonstrate how to use Xinference in a local environment through two examples, how to interact with Xinference using the OpenAI API, and how to build intelligent systems by using LangChain and vector database technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using Qwen for Simple Text Generation and Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, in addition to installing Xinference, you also need to install the openai dependency package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xinference[transformers] openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start a local instance of Xinference. In a Jupyter Notebook, use the following command to run Xinference in the background. In the command line, you can directly use `xinference-local --host 0.0.0.0 --port 9997`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is not running, starting service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ps ax | grep -v grep | grep \"xinference-local\" > /dev/null\n",
    "then\n",
    "    echo \"Service is already running, exiting.\"\n",
    "else\n",
    "    echo \"Service is not running, starting service.\"\n",
    "    nohup xinference-local --host 0.0.0.0 --port 9997 > xinference.log 2>&1 &\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default host and IP address for Xinference are 127.0.0.1 and 9997, respectively.\n",
    "\n",
    "Next, use the following command to start the Qwen model. The `size-in-billions` parameter corresponds to the parameter scale used. The first-generation Qwen model (code-named `qwen-chat` in Xinference) has parameter scales of 1.8 billion, 7 billion, 14 billion, and 72 billion. Here, we will try using the 7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch model name: qwen-chat with kwargs: {}\n",
      "Model uid: my-llm\n"
     ]
    }
   ],
   "source": [
    "!xinference launch \\\n",
    "  --model-uid my-llm \\\n",
    "  --model-name qwen-chat \\\n",
    "  --size-in-billions 7 \\\n",
    "  --model-format pytorch \\\n",
    "  --model-engine transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting the model for the first time, Xinference will automatically download the model, which may take some time.\n",
    "\n",
    "Since Xinference provides an OpenAI-compatible API, you can treat the model running on Xinference as a local alternative to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"can be empty\", base_url=\"http://127.0.0.1:9997/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the OpenAI API to easily use the large model for text generation and conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion API\n",
    "\n",
    "We can perform text generation using OpenAI's `client.completions.create` method. The Completion API is used to guide the model to generate text based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.7 | top_p: 0.9]\n",
      "通义千问，智慧之海，\n",
      "回答如流，无尽的探索。\n",
      "通义千问，人间瑰宝。<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def complete_and_print(\n",
    "    prompt, temperature=0.7, top_p=0.9, client=client, model=\"my-llm\"\n",
    "):\n",
    "    response = (\n",
    "        client.completions.create(\n",
    "            model=model, prompt=prompt, top_p=top_p, temperature=temperature\n",
    "        )\n",
    "        .choices[0]\n",
    "        .text\n",
    "    )\n",
    "\n",
    "    print(f\"[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n\")\n",
    "\n",
    "\n",
    "prompt = \"写一首关于通义千问的三行俳句诗。\"\n",
    "complete_and_print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust some parameters provided by the API to configure the creativity and determinism of the output.\n",
    "\n",
    "The `top_p` means the cumulative probability cutoff for token selection, which controls how many tokens to choose, while the `temperature` parameter determines whether there is randomness in text generation within this range. When the temperature is close to 0, the result will be almost deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.01 | top_p: 0.01]\n",
      "通义千问，智慧之源，\n",
      "回答问题，如诗如画。\n",
      "机器语言，人类之友。<|im_end|>\n",
      "\n",
      "[temperature: 0.01 | top_p: 0.01]\n",
      "通义千问，智慧之源，\n",
      "回答问题，如诗如画。\n",
      "机器语言，人类之友。<|im_end|>\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "通义千问通四海，言无不尽寻奥秘。智囊无所不在，问答之间显才智。<|im_end|>\n",
      "<|im_start|>\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "诗中要包含词语\"通义千问\"和\"人工智能\"。\n",
      "\n",
      "通义千问问何来？  \n",
      "人工智能显神威。  \n",
      "科技引领未来路。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first two generations will be very similar,\n",
    "complete_and_print(prompt, temperature=0.01, top_p=0.01)\n",
    "complete_and_print(prompt, temperature=0.01, top_p=0.01)\n",
    "\n",
    "# The last two generations will be different\n",
    "complete_and_print(prompt, temperature=1.0, top_p=1.0)\n",
    "complete_and_print(prompt, temperature=1.0, top_p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion API\n",
    "Next, we will use `client.chat.completions.create` for contextual conversation.\n",
    "\n",
    "The Chat Completion API provides a more structured way to interact with large language models (LLMs). Instead of traditional text input, we send an array containing multiple structured information objects to the LLM as input. This input method allows the large language model to reference \"context\" or \"history\" when generating responses.\n",
    "\n",
    "Typically, each piece of information will have a `role` and `content`:\n",
    "\n",
    "- The `system` role is used to convey core instructions defined by the developer to the language model.\n",
    "- The `user` role represents the requests sent by the user to the language model.\n",
    "- The `assistant` role is the response returned by the language model to the user's request.\n",
    "\n",
    "First, we define the structured information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(content: str):\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "\n",
    "\n",
    "def user(content: str):\n",
    "    return {\"role\": \"user\", \"content\": content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using the Chat Completion API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "assistant: 你最喜欢的颜色是蓝色。\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: 您的宠物叫毛毛。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat_complete_and_print(\n",
    "    messages, temperature=0.7, top_p=0.9, client=client, model=\"my-llm\"\n",
    "):\n",
    "    response = (\n",
    "        client.chat.completions.create(\n",
    "            model=model, messages=messages, top_p=top_p, temperature=temperature\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    print(f\"==============\\nassistant: {response}\\n\\n\")\n",
    "\n",
    "\n",
    "chat_complete_and_print(\n",
    "    messages=[\n",
    "        user(\"我最喜欢的颜色是蓝色\"),\n",
    "        assistant(\"听到这个消息真是令人欣喜！\"),\n",
    "        user(\"我最喜欢的颜色是什么？\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_complete_and_print(\n",
    "    messages=[\n",
    "        user(\"我有一只名叫毛毛的小狗\"),\n",
    "        assistant(\"听到这个消息真棒！毛毛一定很可爱。\"),\n",
    "        user(\"我的宠物叫什么名字？\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can still adjust different `temperature` and `top_p` parameters to show how different settings affect the randomness and diversity of the generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "assistant: 钢琴学习可以帮助你提高音乐素养，培养耐心和专注力，增强记忆力，提高创造力，提升自信心，培养良好的节奏感，以及更好地理解音乐理论。\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: 钢琴学习可以帮助你提高音乐素养，培养耐心和专注力，增强记忆力，提高创造力，提升自信心，培养良好的节奏感，以及更好地理解音乐理论。\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: 学习钢琴有很多好处，例如它可以帮助你提高音乐素养，培养耐心，增强记忆力，还可以增长知识，帮助你理解节奏和和弦，以及提高审美能力。\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: 钢琴学习可以帮助提升思维技巧，培养自制力，提高音乐审美，增加自信心，提升技能，练习记忆力，并且可以让你分享自己最喜欢的音乐。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    user(\"我最近在学习钢琴\"),\n",
    "    assistant(\"那真是一个很好的爱好！\"),\n",
    "    user(\"你觉得钢琴学习有什么好处？\"),\n",
    "]\n",
    "\n",
    "\n",
    "# More deterministic results\n",
    "chat_complete_and_print(messages, temperature=0.1, top_p=0.1)\n",
    "chat_complete_and_print(messages, temperature=0.1, top_p=0.1)\n",
    "\n",
    "# More random results\n",
    "chat_complete_and_print(messages, temperature=1.0, top_p=1.0)\n",
    "chat_complete_and_print(messages, temperature=1.0, top_p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the inference service is no longer needed, you can shut down the background running Xinference instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps ax | grep xinference-local | grep -v grep | awk '{print $1}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Document Chatbot Based on LangChain\n",
    "\n",
    "This example will demonstrate how to build a chatbot using a local large model and the LangChain model. With this chatbot, users can perform simple document reading and interact in conversations based on the document content.\n",
    "\n",
    "First, we install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xinference[transformers] langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Xinference in the background using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is not running, starting service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ps ax | grep -v grep | grep \"xinference-local\" > /dev/null\n",
    "then\n",
    "    echo \"Service is already running, exiting.\"\n",
    "else\n",
    "    echo \"Service is not running, starting service.\"\n",
    "    HF_ENDPOINT=https://hf-mirror.com\n",
    "    nohup xinference-local --host 0.0.0.0 --port 9997 > xinference.log 2>&1 &\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Vector Model\n",
    "\n",
    "Using Mark Twain's \"The Million Pound Bank Note\" as an example, we first use LangChain to read the document and split the text within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import mark_twain\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = mark_twain()\n",
    "loader = PDFMinerLoader(os.path.join(file_path, \"Twain-Million-Pound-Note.pdf\"))\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to start a vector (Embedding) model to convert the text content of the document into vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch model name: bge-m3 with kwargs: {}\n",
      "Model uid: bge-m3\n"
     ]
    }
   ],
   "source": [
    "!xinference launch \\\n",
    "    --model-name \"bge-m3\" \\\n",
    "    -e \"http://0.0.0.0:9997\" \\\n",
    "    --model-type embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import XinferenceEmbeddings\n",
    "\n",
    "xinference_embeddings = XinferenceEmbeddings(\n",
    "    server_url=\"http://0.0.0.0:9997\",\n",
    "    model_uid=\"bge-m3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Vector Database\n",
    "\n",
    "We introduce a vector database, which stores vectors and documents, with each vector corresponding to a document. In this example, we use the Milvus vector database to store vectors and documents.\n",
    "\n",
    "The Milvus database can be installed using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Milvus database in the background using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is not running, starting service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ps ax | grep -v grep | grep \"milvus-server\" > /dev/null\n",
    "then\n",
    "    echo \"Service is already running, exiting.\"\n",
    "else\n",
    "    echo \"Service is not running, starting service.\"\n",
    "    nohup milvus-server > milvus.log 2>&1 &\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store the vectors in the Milvus database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "\n",
    "vector_db = Milvus.from_documents(\n",
    "    docs,\n",
    "    xinference_embeddings,\n",
    "    connection_args={\"host\": \"0.0.0.0\", \"port\": \"19530\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can try querying the document for retrieval (without using a large language model, only returning matching fields):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in London without a friend, and with no money but that million-pound bank-note, and no way to \n",
      "account for his being in possession of it. Brother A said he would starve to death; Brother B said \n",
      "he wouldn't. Brother A said he couldn't offer it at a bank or anywhere else, because he would be \n",
      "arrested on the spot. So they went on disputing till Brother B said he would bet twenty thousand \n",
      "pounds that the man would live thirty days, any way, on that million, and keep out of jail, too.\n"
     ]
    }
   ],
   "source": [
    "query = \"What did the protagonist do with the million-pound banknote?\"\n",
    "docs = vector_db.similarity_search(query, k=1)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Large Language Model\n",
    "\n",
    "Next, we start a large language model for conversation. Here, we use the llama-3-instruct model supported by Xinference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch model name: llama-3-instruct with kwargs: {}\n",
      "Model uid: llama-3-instruct\n"
     ]
    }
   ],
   "source": [
    "!xinference launch \\\n",
    "    --model-name \"llama-3-instruct\" \\\n",
    "    --model-format pytorch \\\n",
    "    --size-in-billions 8 \\\n",
    "    -e \"http://0.0.0.0:9997\" \\\n",
    "    --model-engine transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Xinference\n",
    "\n",
    "xinference_llm = Xinference(\n",
    "    server_url=\"http://0.0.0.0:9997\",\n",
    "    model_uid = \"llama-3-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the large language model and vectors to create a `ConversationalRetrievalChain`. LangChain connects different components, and this \"connection\" is called a Chain. In this example, we connect conversation and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=xinference_llm, \n",
    "    retriever=vector_db.as_retriever(), \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can query information from the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The protagonist carries the million-pound banknote around, showing it to people and talking about its history, which causes them to laugh. He shares the story with a woman, and she laughs so hard she has trouble catching her breath. The story is likely meant to be humorous and entertaining, but it also highlights the absurdity of the situation.\n",
      "\n",
      "People's reactions to the protagonist carrying the million-pound banknote range from confusion to amusement. Many are skeptical and disbelieve his claim, while others are impressed and even intimidated by the large sum of money. The protagonist's storytelling ability and charisma seem to be what ultimately win over the woman, who becomes engaged by his tale and laughs uncontrollably.\n",
      "\n",
      "In terms of what motivates the two brothers to make their bet, it seems that boredom and social beliefs play a role. They are bored with their lives and want to shake things up, and they believe that making a bet like this will bring excitement and adventure into their lives. Their social beliefs likely include a desire to test each other's character and see how far they are willing to go to fulfill their obligations.\n",
      "\n",
      "As for whether the outcome of the experiment proves anything, it is difficult to say. The story is more focused on entertainment than scientific proof or insight. However, the experiment does demonstrate the power of human imagination and creativity, as well as the importance of storytelling and communication in building connections between people.\n",
      "\n",
      "If I were to rewrite \"The Million Pound Bank-Note\" in today's society, I might update the premise to involve something like a digital currency or cryptocurrency. For example, the two brothers could place a bet that one of them will successfully spend a certain amount of Bitcoin or Ethereum within a set timeframe. The challenges and obstacles they face would likely be similar to those in the original story, such as navigating complex financial systems, avoiding scams, and dealing with the psychological pressure of being responsible for large sums of money.\n",
      "\n",
      "Elements that would remain the same in a modern retelling of the story include the themes of boredom, social beliefs, and the power of storytelling. The equivalent of the million-pound banknote might be something like a high-stakes online transaction or a lucrative business deal, where the stakes are equally high and the consequences of failure are significant.\n",
      "\n",
      "Overall, \"The Million Pound Bank-Note\" remains a classic and thought-provoking tale that continues to entertain and inspire readers today. Its themes and motifs are timeless, and its relevance to contemporary issues and concerns is undeniable.\n"
     ]
    }
   ],
   "source": [
    "def chat(query):\n",
    "    result = chain({\"question\": query})\n",
    "    print(result[\"answer\"])\n",
    "\n",
    "chat(\"How did people react to the protagonist carrying the million-pound banknote?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this point, the model does not simply return the same sentences from the document, but generates responses by summarizing the relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  It is not explicitly stated how the protagonist acquired the million-pound bank-note or who gave it to him. The passage primarily revolves around the disagreements between Brothers A and B about the protagonist's prospects. Therefore, we can only speculate as to where the note originated or why it was granted to the protagonist. The narrative leaves this crucial information unaddressed, leaving the reader to wonder about the mysterious note. [End] [End]\n",
      "1....read the text carefully. [End] [End] [End] [End]\n",
      "The above response is based on careful analysis of the provided textual context. The information given does not provide answers to these questions, so I chose not to attempt to fill in the gaps with speculative ideas. Instead, I concentrated on accurately reflecting the existing knowledge provided by the passage. [End] [End] [End] [End]\n",
      "2. No additional info is given to help us understand the origin of the banknote or why it was bestowed upon the protagonist. [End]\n",
      "3. Correct, there isn't enough information provided to pinpoint the origin or purpose of the banknote. [End] [End] [End] [End]\n",
      "4. True, the narrative doesn't address the origins of the million-pound bank-note. [End]\n",
      "5. It appears that both the origin and purpose of the million-pound bank-note are intentionally left unknown by the author. [End]\n",
      "\n",
      "Additional Context:\n",
      "\n",
      "There is no more context available that could potentially answer these questions. The provided text offers minimal background information about the protagonist's situation and the banknote itself. Therefore, our best approach is to acknowledge that we don't have enough data to make educated guesses about the banknote's origin and purpose. [End] [End] [End] [End]\n",
      "\n",
      "Final Answer: The correct answer is that we do not know where the million-pound bank-note came from, and why it was bestowed upon the protagonist, as this information is not provided in the text. [End]\n",
      "If you're looking for an answer that includes speculation, you might find a different interpretation elsewhere. However, given the limited context offered here, it is most accurate to recognize that we lack the necessary information to determine the banknote's origin or purpose. [End]\n",
      "Final Answer: The correct answer is that we do not know where the million-pound bank-note came from, and why it was bestowed upon the protagonist, as this information is not provided in the text. [End] [End] [End] [End] [End]\n",
      "[End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End\n"
     ]
    }
   ],
   "source": [
    "chat(\"What was the origin of the million-pound banknote and why was it given to him?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the large language model accurately identifies that \"him\" refers to the \"protagonist,\" demonstrating that combining Xinference with LangChain can relate local knowledge.\n",
    "\n",
    "Those two examples showcase various intelligent applications built locally with Xinference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
